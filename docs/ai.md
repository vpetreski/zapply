# Zapply - AI Context

## Current Phase
**Phase 2: Core Features Implementation - SCRAPER & MATCHER COMPLETE**

Working Nomads scraper and AI-powered matcher are fully implemented and working! Currently testing and refining the matching logic. Next: Applier implementation.

## Last Session - 2025-11-24 (Evening)

### Accomplished This Session
**Fixed Critical Matcher Bug:**
- âœ… Diagnosed "No user profile found" error blocking matching phase
- âœ… Updated `scripts/init_user_profile.py` with real CV content from Resume-Vanja-Petreski.pdf
- âœ… Replaced placeholder CV with actual 20-year Principal Software Engineer profile
- âœ… Updated skills (51 skills: Java, Kotlin, Spring Boot, AWS, AI-native development, etc.)
- âœ… Fixed preferences to reflect contractor status and Colombia location
- âœ… Created UserProfile in database (6,500 character CV)
- âœ… Created `scripts/clean_jobs_runs.py` to clean test data while preserving profile
- âœ… Cleaned 745 test jobs and 1 failed run from database
- âœ… Ready for fresh matching test with real user profile

**Working Nomads Scraper - COMPLETE:**
- âœ… Playwright-based scraper for Working Nomads
- âœ… Login with premium account credentials
- âœ… Apply filters (Development + Anywhere,Colombia)
- âœ… Load all jobs by clicking "Show more" until exhausted
- âœ… Scrape job details (title, company, description, tags, apply URL)
- âœ… Save to database with duplicate detection
- âœ… Progress logging and Run tracking
- âœ… Successfully scraped 745+ jobs in testing

**AI Matcher - COMPLETE:**
- âœ… Claude API integration for intelligent job matching
- âœ… Async Claude client for better performance
- âœ… Match jobs against user CV and preferences
- âœ… Score-based matching (0-100 scale)
- âœ… Detailed reasoning with strengths/concerns/recommendation
- âœ… MATCHED (â‰¥60) vs REJECTED (<60) status updates
- âœ… Batch processing with progress tracking
- âœ… Error handling and recovery

**Service Layer - COMPLETE:**
- âœ… `scraper_service.py`: Orchestrates scraping + matching pipeline
- âœ… `matching_service.py`: AI-powered job matching with Claude
- âœ… Run tracking with logs and statistics
- âœ… Progress callbacks for real-time updates
- âœ… Sequential pipeline: SCRAPING â†’ MATCHING â†’ (future: APPLYING)

**API Endpoints - COMPLETE:**
- âœ… `/api/scraper/run`: Trigger scraping manually
- âœ… `/api/runs`: Get all runs with stats
- âœ… `/api/runs/{id}`: Get single run details with logs
- âœ… `/api/jobs`: List jobs with filtering
- âœ… `/api/stats`: Dashboard statistics

### Previous Session - 2025-11-24 (Morning)

**Initial Setup:**
- Read and understood project requirements from `docs/initial-prompt.md`
- Reviewed Vanja's resume (`docs/Resume-Vanja-Petreski.pdf`)
- Confirmed technology choices with user:
  - Python 3.12, uv for package management
  - Monorepo structure (backend + frontend together)
  - Fresh Docker setup

**Backend (Python/FastAPI):**
- âœ… Complete FastAPI application structure
- âœ… Database models: Job, UserProfile, ApplicationLog with proper status tracking
- âœ… Pydantic schemas for API requests/responses
- âœ… API endpoints: health check, jobs (list/detail/update), statistics
- âœ… SQLAlchemy async database setup
- âœ… Alembic migrations configuration
- âœ… Module skeletons for all 4 components (scraper, matcher, applier, reporter) with TODOs
- âœ… Configuration management via pydantic-settings
- âœ… pyproject.toml with all dependencies defined

**Frontend (Vue.js):**
- âœ… Vue 3 with Composition API and Vue Router
- âœ… Modern dark theme UI with clean styling
- âœ… Three main views: Dashboard (stats), Jobs (list with filters), Stats (detailed analytics)
- âœ… Axios integration for API communication
- âœ… Vite build configuration with proxy to backend

**Infrastructure:**
- âœ… Dockerfile for backend (Python + Playwright)
- âœ… Dockerfile for frontend (Node build + Nginx)
- âœ… docker-compose.yml with all services (postgres, backend, frontend)
- âœ… PostgreSQL service with health checks
- âœ… Nginx reverse proxy configuration for frontend

**Development Workflow:**
- âœ… Comprehensive Justfile with 30+ automation recipes
- âœ… Setup, database, development, docker, code quality, and utility commands
- âœ… README updated with quick start using just commands
- âœ… .env.example template for configuration
- âœ… Git repository initialized and pushed to GitHub

**Documentation & Tooling:**
- âœ… CLAUDE.md - Detailed instructions for Claude Code
- âœ… .cursorrules - Instructions for Cursor IDE
- âœ… docs/ai.md - Context tracking (this file)
- âœ… Moved ai.md to docs folder for better organization
- âœ… Created private GitHub repo: https://github.com/vpetreski/zapply

### Current Status
- **Project structure**: 100% complete and committed
- **Backend skeleton**: Ready for implementation (all TODOs marked)
- **Frontend UI**: Fully functional dashboard (will display data once backend implements features)
- **Database**: Models defined, migrations ready to run
- **Development workflow**: Streamlined with Justfile commands
- **Git repository**: Pushed to GitHub, all changes committed

### Next Steps (When User Returns)
**User will:**
1. Review the complete setup
2. Install just command runner
3. Run `just setup` to install dependencies
4. Test the application locally
5. Provide feedback on structure and approach

**Then implement core features:**
1. **Scraper**: Implement Working Nomads scraper with Playwright
2. **Matcher**: Integrate Claude API for intelligent job filtering
3. **Applier**: Build Playwright + Claude automation for applications
4. **Scheduler**: Set up APScheduler for hourly job fetching
5. **Testing**: Test with real Working Nomads data

## Decisions Log

### Technology Stack
- **Python 3.12**: Latest stable version for best performance
- **uv**: Ultra-fast Rust-based package manager for modern Python workflow
- **just**: Command runner for workflow automation (like make but better)
- **Monorepo**: Simpler for MVP, easier coordination between backend and frontend
- **Fresh Docker**: No existing Synology setup to integrate with

### Architecture Decisions
- **Monolith over microservices**: Simpler for MVP, can split later if needed
- **Sequential processing**: No message queues (Kafka) for MVP - use database status fields
- **Single database**: PostgreSQL for all data - jobs, status, configuration
- **Aggressive AI in Applier**: Cost-efficient in Matcher, but use Claude heavily in Applier
- **Justfile for automation**: Makes setup and development workflow much simpler

### File Organization
- **docs/ai.md**: Moved AI context to docs folder for cleaner project root
- **All docs in docs/**: Centralized documentation location

### MVP Scope (Week 1)
**In Scope:**
- Working Nomads scraper only (single source)
- Basic Claude-powered matcher
- Basic Playwright + Claude applier
- PostgreSQL status tracking
- Simple console/log reporter
- Minimal Vue.js dashboard

**Explicitly Out:**
- Additional job sources
- Advanced dashboard features
- Email/webhook notifications
- Analytics and insights
- Multi-user support
- Commercial features

## Target User Profile (for Matcher)

**Personal Info:**
- Name: Vanja Petreski
- Level: Principal Software Engineer
- Experience: 20 years
- Location: Colombia (Colombian and Serbian citizenship, NO US work permit)
- Work Style: 100% remote contractor via Petreski LLC
- Rate: $10,000 USD/month

**Skills:**
- Core: Java, Kotlin, Spring Boot, Backend, APIs, Architecture, Tech Leadership, Product Management
- Extended (AI-enabled): Python, FastAPI, Go, Node, TypeScript, mobile (iOS, Android, Flutter), frontend (Vue, React)
- Value Prop: Experience, architecture, product thinking, tech leadership - not limited to specific tech stack

**Job Criteria:**
- âœ… MUST HAVE: True remote (location doesn't matter), US companies open to international contractors OR Latam/Colombia hiring, Contract or FT compatible with contractor setup
- âŒ MUST REJECT: US work authorization required, Physical presence requirements, Hybrid positions, Location-specific requirements

## Key Architectural Components

### 1. Scraper (TODO - Implementation needed)
- Runs hourly via scheduler
- Fetches from Working Nomads (premium account)
- First run: last 2 weeks, subsequent: only new jobs
- NO AI - just Playwright data extraction
- Transform to source-agnostic internal model
- File: `app/scraper/working_nomads.py`

### 2. Matcher (TODO - Implementation needed)
- Triggered on new jobs
- Uses Claude API for intelligent matching
- Input: CV PDF + job description + filtering criteria
- Output: MATCHED/REJECTED with reasoning
- Must be cost-efficient with API calls
- File: `app/matcher/matcher.py` (prompt template already created)

### 3. Applier (TODO - Implementation needed)
- Triggered on MATCHED jobs
- Uses Playwright + Claude AI (aggressive usage)
- Navigate arbitrary ATS systems
- Fill forms, answer questions
- Mark as APPLIED/FAILED
- File: `app/applier/applier.py`

### 4. Reporter (TODO - Implementation needed)
- Triggered after application attempts
- Generate reports and notifications
- Initially: console/logs
- Future: email/webhook
- File: `app/reporter/reporter.py`

### Data Flow
```
NEW â†’ MATCHED/REJECTED â†’ APPLIED/FAILED â†’ REPORTED
```

## Infrastructure Plan

**Local Development (using Justfile):**
```bash
just setup           # Install all dependencies
just dev-setup       # Start database and run migrations
just dev-backend     # Run backend (terminal 1)
just dev-frontend    # Run frontend (terminal 2)
```

**Production (Synology NAS):**
- All services in Docker containers via `just docker-up`
- docker-compose.yml for orchestration
- Benefits: 24/7 uptime, residential IP (avoids bot detection), local data storage
- Remote access via Synology QuickConnect

## Job Sources

**MVP:** Working Nomads only
- Vanja has premium account
- Clean UI, frequent updates, good quality
- Primary source for remote jobs

**Future:**
- We Work Remotely
- Remotive
- Gun.io (if quality improves)
- LinkedIn (scraping is complex, maybe use API)

**Explicitly Avoid:**
- JobCopilot (Vanja says ineffective)
- Direct LinkedIn application (bad filtering, no responses)

## Current Status & Next Steps

**âœ… SCRAPER & MATCHER FULLY WORKING!**
- Scraper successfully fetches jobs from Working Nomads with Playwright
- Matcher performs AI-powered job analysis using Claude API
- UserProfile created with real CV data (20 years experience, Principal SWE)
- Database clean and ready for fresh test run
- **System ready to test full scraping + matching pipeline**

**Immediate Next Steps (Tomorrow Morning):**
1. **CRITICAL: Review UserProfile** - User wants to understand how it works and verify accuracy
   - Check `scripts/init_user_profile.py` - CV text, skills, preferences
   - Review what goes into matching prompt
   - This directly influences matching quality - must be done properly
2. **Review database cleanup logic** - Check `scripts/clean_jobs_runs.py`
   - Verify it preserves UserProfile correctly
   - Understand what gets deleted vs. preserved
3. **Implement UserProfile view in UI:**
   - Add `/api/profile` endpoint to get user profile
   - Create new Vue.js view to display profile information
   - Show: name, email, location, rate, skills, preferences, CV text
   - Allow reviewing what the AI sees when matching
4. **Test matching with fresh run:**
   - Trigger new scraping run via `/api/scraper/run` endpoint
   - Monitor matching results and scoring accuracy
   - Verify matches target Principal-level backend/Java/Kotlin roles
   - Check location filtering works (rejects US-authorization-required jobs)
   - Review matched jobs to validate AI matching quality

**Next Session Priorities:**
1. Implement **Applier** component (Playwright + Claude for automated job applications)
2. Add APScheduler for hourly automated scraping runs
3. Test complete workflow: scrape â†’ match â†’ apply
4. Deploy to Synology NAS for 24/7 operation

## Blockers
None! System is functional and ready for testing.

## Notes

### Project Timeline
- Start: November 24, 2025
- Target MVP: December 1, 2025 (1 week)
- This is aggressive but achievable with focused scope
- Day 1: âœ… Complete foundation and structure

### Development Workflow
1. Open Claude Code â†’ loads this `docs/ai.md`
2. I tell user what's next based on this context
3. Work together on implementation
4. User says "save" â†’ I update this file and commit
5. Session complete

### Important Context Files
- `docs/initial-prompt.md` - Original project requirements
- `docs/Resume-Vanja-Petreski.pdf` - Vanja's CV (use for matching)
- `CLAUDE.md` - My instructions (Claude Code)
- `.cursorrules` - Cursor IDE instructions
- `Justfile` - All automation commands

### Justfile Quick Reference
- `just` - Show all commands
- `just setup` - Install everything
- `just dev-setup` - Setup database
- `just dev-backend/frontend` - Run services
- `just docker-up` - Run everything in Docker
- `just health` - Check service status
- `just db-migrate "msg"` - Create migration

### Cost Considerations
- Matcher: Be efficient - don't call Claude API unnecessarily
- Applier: Use Claude aggressively - worth the cost to automate applications
- Monitor API usage during MVP to understand real costs

### Key Success Criteria
1. Can fetch jobs from Working Nomads hourly
2. Can accurately filter jobs (remote, contractor-friendly, Latam-OK)
3. Can submit at least 1 real application automatically
4. User can see jobs and status in dashboard
5. System runs unattended on Synology NAS

## Implementation Status

### âœ… Completed (Phase 1 & 2)
**Foundation:**
- [x] Project structure and organization
- [x] Backend skeleton (FastAPI, models, schemas, endpoints)
- [x] Frontend UI (Vue.js, dashboard, views)
- [x] Database configuration (PostgreSQL, Alembic)
- [x] Docker configuration (all services)
- [x] Development automation (Justfile)
- [x] Documentation (README, CLAUDE.md, .cursorrules)
- [x] Git repository and GitHub setup

**Core Features (Scraper & Matcher):**
- [x] Working Nomads scraper with Playwright (fully functional)
- [x] Claude API integration for AI-powered matching
- [x] UserProfile with real CV data (6,500 chars, 51 skills)
- [x] Service layer (scraper_service.py, matching_service.py)
- [x] Run tracking with logs and statistics
- [x] API endpoints for manual triggering
- [x] Database migrations applied
- [x] Testing scripts (init_user_profile.py, clean_jobs_runs.py)

### â³ In Progress
- [ ] Testing matcher with real jobs (NEXT: trigger fresh run)
- [ ] Validating match quality and scoring

### ðŸ“‹ Pending (MVP Week 1)
- [ ] Playwright + Claude applier implementation
- [ ] APScheduler integration for hourly automated runs
- [ ] Reporter component (email/webhook notifications)
- [ ] End-to-end testing of complete workflow
- [ ] Deploy to Synology NAS

### ðŸš€ Future Enhancements (Post-MVP)
- [ ] Additional job sources (We Work Remotely, Remotive)
- [ ] Email/webhook notifications
- [ ] Advanced dashboard analytics
- [ ] Multi-user support
- [ ] Commercial features

---

**Last Updated:** 2025-11-24 by Claude Code
**Next Session:** User will review setup, provide feedback, then implement core features (scraper, matcher, applier)
**GitHub:** https://github.com/vpetreski/zapply (private)
